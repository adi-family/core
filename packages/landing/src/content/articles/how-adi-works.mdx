export const metadata = {
  title: "Under the Hood: How ADI Transforms Tasks into Code",
  description: "A technical deep-dive into ADI's architecture: from GitLab CI orchestration to AI-powered evaluation and autonomous implementation.",
  date: "2025-10-27",
  author: "ADI Engineering Team",
  tags: ["technical", "architecture", "gitlab-ci", "ai-agents"],
  image: "/interview-image.png"
};

# Under the Hood: How ADI Transforms Tasks into Code

Most AI coding tools give you a chatbot. ADI gives you a pipeline. Here's exactly how it works—no marketing fluff, just the technical architecture that powers autonomous task-to-code transformation.

## The Architecture: Three Stages, Zero Human Intervention

ADI runs on GitLab CI runners with a three-stage pipeline architecture:

1. **Prepare** - Workspace sync and environment setup
2. **Execute** - AI evaluation or implementation
3. **Report** - Results upload and merge request creation

Every task flows through this pipeline automatically. No manual triggers, no developer intervention during execution.

## Stage 1: The Evaluation Pipeline

Not every task should be handed to an AI agent. ADI's evaluation pipeline uses a two-phase filter to separate the wheat from the chaff.

### Phase 1: Simple Filter (Quick Gate Check)

Before burning compute time on deep analysis, ADI runs a fast gate check using Claude Sonnet 4.5:

**Immediate Rejection Criteria:**
- Requires manual testing or visual verification
- Needs complex/uncommon third-party integrations
- Involves external services without clear API documentation
- Requires human interaction during execution
- Needs UI/UX testing or visual inspection

**Acceptance Criteria:**
- Can be solved with code changes only
- Has clear, testable success criteria
- Uses common, well-documented integrations
- Can be verified programmatically

This filter takes seconds and costs pennies. If a task fails here, the pipeline stops—saving both time and API costs.

### Phase 2: Deep Agentic Evaluation

Tasks that pass the simple filter enter deep evaluation. This is where ADI gets interesting.

Using the Claude Agent SDK, ADI spawns an autonomous agent with read-only access to your codebase. The agent:

1. **Explores the repository** using Glob, Grep, and Read tools
2. **Analyzes existing patterns** in your code
3. **Maps database schemas** and architectural conventions
4. **Identifies required context** for implementation
5. **Generates concrete agent instructions** for the implementation phase

The output is structured JSON containing:

```json
{
  "can_implement": true,
  "confidence": 85,
  "agent_instructions": {
    "required_context_files": ["src/admin.ts:45-120"],
    "suggested_steps": ["Add AWS SDK integration", "Update config schema"],
    "follow_patterns_from": ["src/services/storage.ts:30-80"]
  },
  "missing_information": [],
  "blockers": [],
  "risks": ["API rate limits on S3 operations"]
}
```

Plus a detailed markdown report documenting:
- Relevant files with specific line numbers
- Database schema changes required
- Existing implementation patterns to follow
- Testing approach

Currently, **60% of evaluated tasks** pass both phases and move to implementation. The other 40% either lack clarity, require human judgment, or have technical blockers.

## Stage 2: The Implementation Pipeline

Tasks approved by evaluation enter the implementation pipeline. This is where code gets written.

### Multi-Agent Support (Intelligently Selected)

ADI doesn't lock you into a single AI model. We support:

- **Claude Sonnet 4.5** (via Anthropic API)
- **OpenAI Codex** (GPT-4 based)
- **Google Gemini**
- Any model supported by Claude Agent SDK

ADI automatically selects the appropriate model based on task characteristics. In the future, users will have direct control over model selection, including per-task complexity routing.

### Autonomous Execution

The implementation agent receives:
- The task description
- Agent instructions from the evaluation phase
- Full read/write access to workspace repositories
- Allowed tools: Bash, Read, Write, Edit, Glob, Grep

What happens next:

1. Agent explores the codebase using the pre-mapped context files
2. Implements changes following existing patterns
3. Runs tests, type checking, and linting
4. Iterates on failures automatically
5. Documents changes in structured format

The agent operates in `acceptEdits` permission mode—it can modify files, run commands, and commit changes without human approval during execution.

### Self-Verification

ADI doesn't just write code and hope it works. Every implementation goes through:

- **Type checking** (if TypeScript/typed language)
- **Test execution** (unit, integration, whatever exists)
- **Linting** (following project standards)
- **Build verification** (ensuring the project still compiles)

If any verification step fails, the agent iterates. If it can't fix the issue after multiple attempts, the pipeline fails gracefully and reports the blocker.

## Stage 3: The Report Pipeline

Once implementation completes (or fails), the report stage:

1. **Pushes changes** to a new branch in GitLab
2. **Creates a merge request** with detailed description
3. **Uploads artifacts** (logs, usage metrics, evaluation reports)
4. **Updates task status** via API

Merge requests include:
- Summary of changes made
- Test results
- Agent iteration count and cost breakdown
- Link back to the original task

## Real-World Example: AWS Bucket Integration

Let's trace a real task through the pipeline:

**Task:** "Implement AWS S3 bucket integration for file uploads"

**Evaluation Phase:**
- Simple filter: ✓ Pass (clear API documentation, no visual verification)
- Deep evaluation: Agent explores existing file upload code, maps storage patterns, identifies AWS SDK as dependency
- Output: `can_implement: true, confidence: 82%`

**Implementation Phase:**
- Agent adds `@aws-sdk/client-s3` dependency
- Follows patterns from existing storage service
- Implements upload/download methods
- Adds error handling based on project conventions
- Writes integration tests mirroring other storage tests
- Verifies build and tests pass

**Result:** Merge request created in 8 minutes, cost: $0.47

## The Secret Sauce

What makes ADI different isn't just orchestration or AI agents. It's the combination of:

1. **Task-specific context mapping** before implementation starts
2. **Pattern recognition** from existing codebase
3. **Iterative self-correction** without human intervention
4. **Structured verification** at every step
5. **Full GitLab CI integration** for seamless workflow

Traditional AI assistants wait for you to feed them context. ADI finds the context itself, follows your existing patterns, and verifies its own work.

## The Tech Stack

- **Orchestration:** GitLab CI/CD runners
- **Runtime:** Bun (for worker scripts)
- **AI Integration:** Claude Agent SDK
- **Models:** Anthropic Claude, OpenAI Codex, Google Gemini
- **Version Control:** GitLab (GitHub coming Q2 2026)
- **Task Sources:** Jira, Linear, GitLab Issues, GitHub Issues, custom APIs

## What Happens When Things Go Wrong?

ADI fails gracefully:

- Evaluation rejects unclear tasks → no wasted implementation time
- Implementation fails verification → detailed error logs in MR
- Pipeline errors → status updated, no silent failures
- API rate limits → automatic retry with backoff

Every failure creates an artifact trail. You always know what happened and why.

## Coming Soon

We're actively working on:

- **User-controlled model selection** (let users pick Claude vs Codex vs Gemini)
- **Complexity-based routing** (simple tasks → fast models, complex → premium)
- **GitHub Actions support** (parity with GitLab CI)
- **Self-hosted deployments** (run ADI infrastructure on your own servers)

---

*ADI is currently in public beta. Want to see the pipeline in action? [Try ADI](https://the-ihor.com) with $100 free credit.*
